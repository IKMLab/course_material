{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char-RNN-文本生成\n",
    "## 教學目標\n",
    "使用 RNN 弄出一個基本的生成文字模型，幫助初學者上手 RNN\n",
    "\n",
    "## 適用對象\n",
    "適用於已經學過 PyTorch 基本語法的人\n",
    "\n",
    "## 執行方法\n",
    "在 Jupyter notebook 中，選取想要執行的區塊後，使用以下其中一種方法執行\n",
    "\n",
    "- 上方工具列中，按下 Cell < Run Cells 執行\n",
    "- 使用快捷鍵 Shift + Enter 執行\n",
    "\n",
    "## 大綱\n",
    "- [載入資料](#載入資料)\n",
    "- [前處理](#前處理)\n",
    "- [建立字典](#建立字典)\n",
    "- [超參數](#超參數)\n",
    "- [資料分批](#資料分批)\n",
    "- [模型設計](#模型設計)\n",
    "- [訓練](#訓練)\n",
    "- [生成](#生成)\n",
    "\n",
    "## 檔案來源\n",
    "- [Kaggle HC 新聞資料集](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)\n",
    "- 下載後請放到路徑 `專案資料夾/data/old-newspaper.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.utils.rnn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import opencc\n",
    "\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入資料\n",
    "- 請務必先[下載](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)資料後將資料放置到 `data` 資料夾之下\n",
    "- `tsv` 檔案類似 `csv`，只是用 `\\t` 做分隔符號\n",
    "- 資料內容包含\n",
    "\n",
    "|欄位|意義|資料型態|\n",
    "|-|-|-|\n",
    "|`Language`|語系|文字（類別）|\n",
    "|`Source`|新聞來源|文字|\n",
    "|`Date`|時間|文字|\n",
    "|`Text`|文字內容|文字|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path + '/old-newspaper.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前處理\n",
    "- 訓練目標為生成繁體中文字\n",
    "    - 所以只考量繁體中文的資料\n",
    "    - 類別為 `Chinese (Traditional)`\n",
    "    - 共約 333735 筆\n",
    "- 資料長度不一\n",
    "    - 畫出長度分佈圖\n",
    "    - 計算長度四分位數、最小值、最大值\n",
    "    - 為了方便訓練，只考慮長度介於 60~200 的新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Language'] == 'Chinese (Traditional)'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Language'] == 'Chinese (Traditional)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['Text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "sns.countplot(df['len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['len'].describe())\n",
    "print(df[df['len'] <= 200].shape[0])\n",
    "print(df[df['len'] >= 60].shape[0])\n",
    "print(df[(df['len'] >= 60) & (df['len'] <= 200)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['len'] >= 60) & (df['len'] <= 200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立字典\n",
    "- 無法直接利用純文字進行計算\n",
    "- 將所有文字轉換成數字\n",
    "- 字典大小約為 `7000`\n",
    "- 特殊字\n",
    "    - '&lt;pad&gt;'\n",
    "        - 每個 batch 所包含的句子長度不同\n",
    "        - 將長度使用 '&lt;pad&gt;' 補成 batch 中最大值者\n",
    "    - '&lt;eos&gt;'\n",
    "        - 指定生成的結尾\n",
    "        - 沒有 '&lt;eos&gt;' 會不知道何時停止生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {}\n",
    "id_to_char = {}\n",
    "\n",
    "char_to_id['<pad>'] = 0\n",
    "char_to_id['<eos>'] = 1\n",
    "id_to_char[0] = '<pad>'\n",
    "id_to_char[1] = '<eos>'\n",
    "\n",
    "for char in set(df['Text'].str.cat()):\n",
    "    ch_id = len(char_to_id)\n",
    "    char_to_id[char] = ch_id\n",
    "    id_to_char[ch_id] = char\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "print('字典大小: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_id_list'] = df['Text'].apply(lambda text: [char_to_id[char] for char in list(text)] + [char_to_id['<eos>']])\n",
    "\n",
    "df[['Text', 'char_id_list']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超參數\n",
    "\n",
    "|超參數|意義|數值|\n",
    "|-|-|-|\n",
    "|`batch_size`|單一 batch 的資料數|64|\n",
    "|`epochs`|總共要訓練幾個 epoch|10|\n",
    "|`embed_dim`|文字的 embedding 維度|50|\n",
    "|`hidden_dim`|LSTM 中每個時間的 hidden state 維度|50|\n",
    "|`lr`|Learning Rate|0.001|\n",
    "|`grad_clip`|為了避免 RNN 出現梯度爆炸問題，將梯度限制範圍|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "embed_dim = 50\n",
    "hidden_dim = 50\n",
    "lr = 0.001\n",
    "grad_clip = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料分批\n",
    "- 使用 `torch.utils.data.Dataset` 建立資料產生的工具 `dataset`\n",
    "- 再使用 `torch.utils.data.DataLoader` 對資料集 `dataset` 隨機抽樣並作為一個 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.sequences.iloc[index][:-1]\n",
    "        y = self.sequences.iloc[index][1:]\n",
    "        return x, y\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    batch_x = [torch.tensor(data[0]) for data in batch]\n",
    "    batch_y = [torch.tensor(data[1]) for data in batch]\n",
    "    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])\n",
    "    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])\n",
    "    \n",
    "    pad_batch_x = torch.nn.utils.rnn.pad_sequence(batch_x,\n",
    "                                                  batch_first=True,\n",
    "                                                  padding_value=char_to_id['<pad>'])\n",
    "    \n",
    "    pad_batch_y = torch.nn.utils.rnn.pad_sequence(batch_y,\n",
    "                                                  batch_first=True,\n",
    "                                                  padding_value=char_to_id['<pad>'])\n",
    "    \n",
    "    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(df['char_id_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型設計\n",
    "\n",
    "## 執行順序\n",
    "1. 將句子中的所有字轉換成 embedding\n",
    "2. 按照句子順序將 embedding 丟入 LSTM\n",
    "3. LSTM 的輸出再丟給 LSTM，可以接上更多層\n",
    "4. 最後的 LSTM 所有時間點的輸出丟進一層 Fully Connected\n",
    "5. 輸出結果所有維度中的最大者即為下一個字\n",
    "\n",
    "## 損失函數\n",
    "因為是類別預測，所以使用 Cross Entropy\n",
    "\n",
    "## 梯度更新\n",
    "使用 Adam 演算法進行梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=embed_dim,\n",
    "                                            padding_idx=char_to_id['<pad>'])\n",
    "        \n",
    "        self.rnn_layer1 = torch.nn.LSTM(input_size=embed_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        self.rnn_layer2 = torch.nn.LSTM(input_size=hidden_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=hidden_dim,\n",
    "                                                          out_features=hidden_dim),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=hidden_dim,\n",
    "                                                          out_features=vocab_size))\n",
    "        \n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "    \n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        batch_x = self.embedding(batch_x)\n",
    "        \n",
    "        batch_x = torch.nn.utils.rnn.pack_padded_sequence(batch_x,\n",
    "                                                          batch_x_lens,\n",
    "                                                          batch_first=True,\n",
    "                                                          enforce_sorted=False)\n",
    "        \n",
    "        batch_x, _ = self.rnn_layer1(batch_x)\n",
    "        batch_x, _ = self.rnn_layer2(batch_x)\n",
    "        \n",
    "        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x,\n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        batch_x = self.linear(batch_x)\n",
    "        \n",
    "        return batch_x\n",
    "    \n",
    "    def generator(self, start_char, max_len=200):\n",
    "        \n",
    "        char_list = [char_to_id[start_char]]\n",
    "        \n",
    "        next_char = None\n",
    "        \n",
    "        while len(char_list) < max_len: \n",
    "            x = torch.LongTensor(char_list).unsqueeze(0)\n",
    "            x = self.embedding(x)\n",
    "            _, (ht, _) = self.rnn_layer1(x)\n",
    "            _, (ht, _) = self.rnn_layer2(ht)\n",
    "            y = self.linear(ht)\n",
    "            \n",
    "            next_char = np.argmax(y.numpy())\n",
    "            \n",
    "            if next_char == char_to_id['<eos>']:\n",
    "                break\n",
    "            \n",
    "            char_list.append(next_char)\n",
    "            \n",
    "        return [id_to_char[ch_id] for ch_id in char_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = CharRNN(vocab_size,\n",
    "                embed_dim,\n",
    "                hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'], reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練\n",
    "1. 最外層的 `for` 迴圈控制 `epoch`\n",
    "    1. 內層的 `for` 迴圈透過 `data_loader` 取得 batch\n",
    "        1. 丟給 `model` 進行訓練\n",
    "        2. 預測結果 `batch_pred_y` 跟真正的答案 `batch_y` 進行 Cross Entropy 得到誤差 `loss`\n",
    "        3. 使用 `loss.backward` 自動計算梯度\n",
    "        4. 使用 `torch.nn.utils.clip_grad_value_` 將梯度限制在 `-grad_clip` &lt; &lt; `grad_clip` 之間\n",
    "        5. 使用 `optimizer.step()` 進行更新（back propagation）\n",
    "2. 每 `1000` 個 batch 就輸出一次當前的 loss 觀察是否有收斂的趨勢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "i = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    for batch_x, batch_y, batch_x_lens, batch_y_lens in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        batch_pred_y = model(batch_x.to(device), batch_x_lens.to(device))\n",
    "        \n",
    "        batch_pred_y = batch_pred_y.view(-1, vocab_size)\n",
    "        batch_y = batch_y.view(-1).to(device)\n",
    "        \n",
    "        loss = criterion(batch_pred_y, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        i+=1\n",
    "        if i%1000==0:\n",
    "            print('epoch: {}, step: {}, loss: {}'.format(epoch, i, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成\n",
    "使用 `model.generator` 並給予一個起始文字進行自動生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = model.cpu()\n",
    "    print(model.generator('網'))\n",
    "    print(model.generator('地'))\n",
    "    print(model.generator('公'))\n",
    "    print(model.generator('哈'))\n",
    "    print(model.generator('神'))\n",
    "    print(model.generator('次'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
