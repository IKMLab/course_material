{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT tutorial using Hugging Face\n",
    "## æ•™å­¸ç›®æ¨™\n",
    "åˆ©ç”¨ Hugging Face å¥—ä»¶å¿«é€Ÿä½¿ç”¨ BERT æ¨¡å‹ä¾†é€²è¡Œä¸‹æ¸¸ä»»å‹™è¨“ç·´\n",
    "- å–®ä¸€å¥å‹åˆ†é¡ä»»å‹™ (single-sentence text classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é©ç”¨å°è±¡\n",
    "å·²ç¶“æœ‰åŸºæœ¬çš„æ©Ÿå™¨å­¸ç¿’çŸ¥è­˜ï¼Œä¸”æ“æœ‰ Pythonã€`numpy`ã€`pandas`ã€`scikit-learn` ä»¥åŠ `PyTorch` åŸºç¤çš„å­¸ç”Ÿã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é Pythonï¼Œè«‹åƒè€ƒ [python-å…¥é–€èªæ³•](./python-å…¥é–€èªæ³•.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é `pandas`ï¼Œè«‹åƒè€ƒ [pandas-åŸºæœ¬åŠŸèƒ½](./pandas-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é `numpy`ï¼Œè«‹åƒè€ƒ [numpy-åŸºæœ¬åŠŸèƒ½](./numpy-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é `scikit-learn`ï¼Œè«‹åƒè€ƒ [scikit-learn-åŸºæœ¬åŠŸèƒ½](./scikit-learn-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é  `PyTorch` ï¼Œè«‹åƒè€ƒ [PyTorch-åŸºæœ¬åŠŸèƒ½](./PyTorch-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸éå¦‚ä½•ä½¿ç”¨ `PyTorch` å»ºç«‹è‡ªç„¶èªè¨€è™•ç†åºåˆ—æ¨¡å‹ï¼Œè«‹åƒè€ƒ [NN-ä¸­æ–‡æ–‡æœ¬åˆ†é¡](./NN-ä¸­æ–‡æ–‡æœ¬åˆ†é¡.ipynb) æ•™å­¸ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT ç°¡æ˜“ä»‹ç´¹\n",
    "### Word embeddings çš„å•é¡Œ\n",
    "![Imgur](https://i.imgur.com/h6U5k41.png)\n",
    "- æ¯å€‹å–®è©çš„æ„æ€åœ¨ä¸åŒçš„å ´åˆä¸‹æ‡‰è©²æœ‰ä¸åŒçš„æ„ç¾©è¡¨é”\n",
    "- æˆ‘å€‘å¯ä»¥åˆ©ç”¨ RNN ä½œç‚ºèªè¨€æ¨¡å‹ï¼Œé€éèªè¨€æ¨¡å‹çš„è¼¸å…¥èˆ‡è¼¸å‡ºçš„è™•ç†ä¾†ç”¢ç”Ÿèƒ½å¤ ç†è§£ä¸Šä¸‹æ–‡èªæ„çš„ contextual embeddings\n",
    "    - Language model: èªè¨€æ¨¡å‹ï¼Œè—‰ç”±ä¼°è¨ˆ(æˆ–æœ€ä½³åŒ–)ä¸€æ•´å€‹åºåˆ—çš„ç”Ÿæˆæ©Ÿç‡ä¾†è¼¸å‡ºå­—è©çš„æ¨¡å‹\n",
    "        - å¯ä»¥åƒè€ƒ [language model çš„è©³ç´°æ•™å­¸](https://youtu.be/LheoxKjeop8?t=50)\n",
    "- è—‰ç”±æ­¤ç¨®åšæ³•ï¼Œæˆ‘å€‘å¯ä»¥å°‡å–®è©èªæ„çš„ word embeddings è½‰æ›ç‚ºå…·æœ‰ä¸Šä¸‹æ–‡èªæ„çš„ contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ‰€ä»¥ä»€éº¼æ˜¯ BERT?\n",
    "- è«‹åƒè€ƒç†è«–å±¤é¢çš„è©³ç´°æ•™å­¸ ([å½±ç‰‡é€£çµ](https://www.youtube.com/watch?v=gh0hewYkjgo))\n",
    "- æƒ³é€²è¡Œ PyTorch çš„ BERT å¯¦ä½œä¾†ç²å¾—æ·±å…¥ç†è§£å¯ä»¥åƒè€ƒ ([ç¶²èªŒé€£çµ](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html))\n",
    "- ä¹Ÿå¯ä»¥åƒè€ƒ Jay Alammar çš„ The Illustrated BERT ([ç¶²èªŒé€£çµ](https://jalammar.github.io/illustrated-bert/))\n",
    "- ä¹Ÿå¯ä»¥åƒè€ƒåŸå§‹è«–æ–‡ ([è«–æ–‡é€£çµ](https://www.aclweb.org/anthology/N19-1423/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT çš„ Pre-training å’Œ Fine-tuning èˆ‡å…ˆå‰æ–¹æ³•æ¯”è¼ƒ\n",
    "![Imgur](https://i.imgur.com/qfLhUaG.png)\n",
    "- Pre-training å·²ç¶“æ˜¯ NLP é ˜åŸŸä¸­ä¸å¯æˆ–ç¼ºçš„æ–¹æ³•\n",
    "- åƒ BERT é€™é¡åŸºæ–¼ Transformers çš„[æ¨¡å‹éå¸¸å¤š](http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf)ï¼Œå¯ä»¥å‰å¾€ [Hugging Face models](https://huggingface.co/models) ä¸€è¦½ç©¶ç«Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face ä»‹ç´¹\n",
    "- ğŸ¤— Hugging Face æ˜¯å°ˆé–€æä¾›è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸçš„å‡½å¼åº«\n",
    "- å…¶å‡½å¼åº«æ”¯æ´ PyTorch å’Œ TensorFlow\n",
    "- ğŸ¤— Hugging Face çš„ä¸»è¦å¥—ä»¶ç‚º:\n",
    "    1. Transformers ([é€£çµ](https://huggingface.co/transformers/index.html))\n",
    "    - æä¾›äº†ç¾ä»Šæœ€å¼·å¤§çš„è‡ªç„¶èªè¨€è™•ç†æ¨¡å‹ï¼Œä½¿ç”¨ä¸Šéå¸¸å½ˆæ€§ä¸”æ–¹ä¾¿\n",
    "    2. Tokenizers ([é€£çµ](https://huggingface.co/docs/tokenizers/python/latest/))\n",
    "    - è®“ä½ å¯ä»¥å¿«é€Ÿåšå¥½ BERT ç³»åˆ—æ¨¡å‹ tokenization\n",
    "    3. Datasets ([é€£çµ](https://huggingface.co/docs/datasets/))\n",
    "    - æä¾›å¤šç¨®è‡ªç„¶èªè¨€è™•ç†ä»»å‹™çš„è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‹¥æ²’æœ‰å®‰è£ transformers å’Œ datasets å¥—ä»¶ï¼Œè«‹å–æ¶ˆä»¥ä¸‹è¨»è§£ä¸¦åŸ·è¡Œ\n",
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch çš„ç‰ˆæœ¬ç‚º: 1.7.1\n",
      "Hugging Face Transformers çš„ç‰ˆæœ¬ç‚º: 4.5.1\n",
      "Hugging Face Datasets çš„ç‰ˆæœ¬ç‚º: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# 1. ç¢ºèªæ‰€éœ€å¥—ä»¶çš„ç‰ˆæœ¬\n",
    "\n",
    "import torch\n",
    "print(\"PyTorch çš„ç‰ˆæœ¬ç‚º: {}\".format(torch.__version__))\n",
    "\n",
    "import transformers\n",
    "print(\"Hugging Face Transformers çš„ç‰ˆæœ¬ç‚º: {}\".format(transformers.__version__))\n",
    "\n",
    "import datasets\n",
    "print(\"Hugging Face Datasets çš„ç‰ˆæœ¬ç‚º: {}\".format(datasets.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. è¼‰å…¥å…¶ä»–æ‰€éœ€å¥—ä»¶\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path # (Python3.4+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å–®ä¸€å¥å‹åˆ†é¡ä»»å‹™ (single-sentence text classification)\n",
    "## æº–å‚™è³‡æ–™é›† (éœ€å…ˆä¸‹è¼‰)\n",
    "æˆ‘å€‘ä½¿ç”¨ IMDb reviews è³‡æ–™é›†ä½œç‚ºç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‹¥æ²’æœ‰å®‰è£ wget å¥—ä»¶ï¼Œè«‹å–æ¶ˆä»¥ä¸‹è¨»è§£ä¸¦åŸ·è¡Œ\n",
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/dean/.local/share/virtualenvs/nlp-tutorial-pytorch-S7P7VhMA/lib/python3.8/site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "# 3. ä¸‹è¼‰ IMDb è³‡æ–™é›†\n",
    "\n",
    "import wget\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filename = wget.download(url, out='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‹¥æ²’æœ‰å®‰è£ tarfile å¥—ä»¶ï¼Œè«‹å–æ¶ˆä»¥ä¸‹è¨»è§£ä¸¦åŸ·è¡Œ\n",
    "# !pip install tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. è§£å£“ç¸® IMDb è³‡æ–™é›†\n",
    "\n",
    "import tarfile\n",
    "\n",
    "# æŒ‡å®šæª”æ¡ˆä½ç½®ï¼Œä¸¦è§£å£“ç¸® .gz çµå°¾çš„å£“ç¸®æª”\n",
    "tar = tarfile.open('aclImdb_v1.tar.gz', 'r:gz')\n",
    "tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¥ä¸‹ä¾†æˆ‘å€‘è¦é€²è¡Œè³‡æ–™å‰è™•ç†\n",
    "ä½†é¦–å…ˆè¦è§€å¯Ÿè§£å£“ç¸®å¾Œçš„è³‡æ–™å¤¾çµæ§‹:\n",
    "```\n",
    "aclImdb---\n",
    "        |--train\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--test\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--imdb.vocab\n",
    "        |--imdbEr.text\n",
    "        |--README\n",
    "```\n",
    "å…¶ä¸­ train å’Œ test è³‡æ–™å¤¾ä¸­åˆ†åˆ¥åˆæœ‰ neg å’Œ pos å…©ç¨®è³‡æ–™å¤¾\n",
    "\n",
    "æˆ‘å€‘è¦é‡å°é€™å…©å€‹ç›®æ¨™è³‡æ–™å¤¾é€²è¡Œè™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. å‰è™•ç† IMDb è³‡æ–™ (å®šç¾© function)\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        # åˆ©ç”¨iterdir() ä¾†åˆ—å‡ºè³‡æ–™å¤¾åº•ä¸‹çš„æ‰€æœ‰æª”æ¡ˆï¼Œæ­¤åŠŸèƒ½ç­‰åŒæ–¼ os.path.listdir()\n",
    "        # ä½¿ç”¨ glob çš„èªæ³•åˆ†å–å¾—å‰¯æª”åç‚º .txt çš„æª”æ¡ˆ\n",
    "        for text_file in (split_dir/label_dir).glob(\"*.txt\"):\n",
    "            # read_text() æ˜¯ Pathlibçš„å¥½ç”¨åŠŸèƒ½\n",
    "            tmp_text = text_file.read_text()\n",
    "            # å°‡è®€åˆ°çš„æ–‡å­— append åˆ°æˆ‘å€‘äº‹å…ˆå®šç¾©çš„ list ä¸­\n",
    "            texts.append(tmp_text)\n",
    "            # å°‡è³‡æ–™å¤¾æ¨™ç±¤ä½œç‚º labelï¼Œä¸¦ append åˆ°æˆ‘å€‘äº‹å…ˆå®šç¾©çš„ list ä¸­\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)\n",
    "    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. å‰è™•ç† IMDb è³‡æ–™ (åŸ·è¡Œ)\n",
    "\n",
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ‡åˆ†è¨“ç·´è³‡æ–™ï¼Œä¾†åˆ†å‡º validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ä½¿ç”¨ train_test_split ä¾†åˆ‡å‡º validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# è¨­ç«‹éš¨æ©Ÿç¨®å­ä¾†æ§åˆ¶éš¨æ©Ÿéç¨‹\n",
    "random_seed = 42\n",
    "\n",
    "# è¨­å®šè¦åˆ†å‡ºå¤šå°‘æ¯”ä¾‹çš„ validation data\n",
    "valid_ratio = 0.2\n",
    "\n",
    "# ä½¿ç”¨ train_test_split ä¾†åˆ‡åˆ†è³‡æ–™\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, \n",
    "    train_labels,\n",
    "    test_size=valid_ratio, \n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¼¸å…¥ BERT çš„å‰è™•ç†\n",
    "![Imgur](https://i.imgur.com/3C7xDlf.png)\n",
    "(åœ–ç‰‡ä¾†æº: BERT [åŸå§‹è«–æ–‡](https://www.aclweb.org/anthology/N19-1423/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- æ–·å­—çš„éƒ¨ä»½ä»¥ DistilBERT (Sanh et al., 2019) çš„ tokenizer ç‚ºä¾‹\n",
    "- Hugging Face çš„ tokenizer å¯ä»¥ç›´æ¥å¹«ä½ è‡ªå‹•å°‡è³‡æ–™è½‰æ›æˆ BERT çš„è¼¸å…¥å‹å¼ (ä¹Ÿå°±æ˜¯åŠ å…¥[CLS]å’Œ[SEP] tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face AutoTokenizer\n",
    "- ä½¿ç”¨ AutoTokenizer æ­é… Hugging Face models çš„åç¨±å¯ä»¥ç›´æ¥å‘¼å«ä½¿ç”¨\n",
    "- èˆ‰ä¾‹:\n",
    "    - transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    - ç­‰åŒæ–¼ transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "- [é»é€™è£¡ä¾†æŸ¥çœ‹Hugging Face models çš„åç¨±](https://huggingface.co/transformers/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. è¼‰å…¥ tokenizer\n",
    "\n",
    "# åœ¨ Hugging Face å¥—ä»¶ä¸­å¯ä½¿ç”¨ .from_pretrained() çš„æ–¹æ³•ä¾†å°å…¥é è¨“ç·´æ¨¡å‹\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. åˆ†åˆ¥å°‡3ç¨®è³‡æ–™ (train/valid/test) åš tokenization\n",
    "# truncation ä»£è¡¨ä¾ç…§ max_length é€²è¡Œåºåˆ—é•·åº¦çš„è£åˆ‡\n",
    "# max_length å¯ä»¥åœ¨ tokenizer çš„ parameters ä¸­é€²è¡Œè¨­å®š\n",
    "# å¦‚æœæ²’æœ‰æŒ‡å®š max_lengthï¼Œå‰‡ä¾ç…§æ‰€ä½¿ç”¨çš„æ¨¡å‹çš„åºåˆ—æœ€å¤§é•·åº¦\n",
    "# padding ç‚º True è¡¨ç¤ºæœƒå°‡åºåˆ—é•·åº¦è£œé½Šè‡³è©² batch çš„æœ€å¤§é•·åº¦ (æ¬²çŸ¥è©³æƒ…è«‹æŸ¥çœ‹ source code)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. æŸ¥çœ‹ max_length\n",
    "\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID of [CLS] token is 101.\n",
      "The ID of [SEP] token is 102.\n"
     ]
    }
   ],
   "source": [
    "# 11. æŸ¥çœ‹ [CLS] token å’Œ [SEP] token åœ¨å­—å…¸ä¸­çš„ ID\n",
    "\n",
    "print(\"The ID of [CLS] token is {}.\".format(tokenizer.vocab[\"[CLS]\"]))\n",
    "print(\"The ID of [SEP] token is {}.\".format(tokenizer.vocab[\"[SEP]\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æª¢æŸ¥ tokenization å¾Œçš„çµæœ\n",
    "- ä½¿ç”¨ Hugging Face tokenizer é€²è¡Œ tokenization å¾Œçš„çµæœæ˜¯ä¸€å€‹ dict\n",
    "- é€™å€‹ dict çš„ keys åŒ…å« 'input_ids' å’Œ 'attention_mask'\n",
    "- input_ids: åŸæœ¬å¥å­ä¸­çš„æ¯å€‹å­—è©è¢«æ–·è©å¾Œè½‰æ›æˆå­—å…¸çš„ ID\n",
    "    - æ³¨æ„!! tokenizer å°å°çš„å‹•ä½œå·²ç¶“å¹«ä½ å®Œæˆäº†æ–·è©å’Œ word to ID çš„è½‰æ›\n",
    "- attention_mask: tokenization å¾Œå¥å­ä¸­åŒ…å«æ–‡å­—çš„éƒ¨åˆ†ç‚º 1ï¼Œpadding çš„éƒ¨åˆ†ç‚º 0\n",
    "    - å¯ä»¥æƒ³åƒæˆæ¨¡å‹éœ€è¦æŠŠæ³¨æ„åŠ›æ”¾åœ¨æœ‰æ–‡å­—çš„ä½ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[101, 1045, 2442, 6449, 1010, 1045, 2001, 2028, 1997, 1996, 15315, 23606, 6558, 2040, 28179, 13224, 2023, 2265, 2077, 4659, 2151, 2592, 2001, 4487, 11393, 26972, 2055, 2009, 1012, 1045, 4340, 2008, 2009, 2001, 2183, 2000, 2022, 1037, 10036, 6714, 1011, 2125, 8546, 2011, 8923, 1040, 1012, 5405, 26974, 1996, 2128, 13535, 2239, 1011, 23967, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2001, 3308, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 4405, 3727, 2019, 6581, 8605, 2588, 1996, 7193, 1012, 1996, 23661, 2003, 28851, 999, 1997, 2607, 1010, 28223, 18667, 2290, 8244, 2097, 2424, 3209, 26275, 1999, 1996, 5436, 1010, 2029, 2003, 4208, 2006, 1996, 2458, 1997, 1996, 22330, 7811, 2015, 2077, 1996, 2034, 2162, 1012, 1006, 5388, 2086, 2077, 1996, 2824, 1997, 1996, 18667, 2290, 4405, 1007, 1012, 1996, 4405, 2036, 4473, 2005, 24159, 1010, 4415, 10886, 2049, 5436, 1998, 4784, 1999, 1996, 2034, 2112, 1997, 1996, 2792, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2123, 1005, 1056, 2022, 13534, 1024, 1000, 6178, 14735, 1000, 2003, 2025, 18667, 2290, 1012, 2057, 2024, 3591, 2007, 2019, 10047, 16862, 3512, 1010, 18439, 3689, 20384, 2011, 26422, 1010, 15236, 1010, 1998, 6801, 4784, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 9179, 9319, 18667, 2290, 1005, 1055, 4781, 1025, 2358, 27914, 2480, 1998, 17103, 2024, 3432, 2004, 24826, 15683, 1012, 17103, 1005, 13954, 1997, 3312, 4205, 2050, 1010, 4427, 2011, 19330, 15530, 1005, 13954, 1997, 2520, 1010, 3957, 1037, 6919, 12185, 1997, 2520, 1005, 1055, 14779, 2269, 1012, 2358, 27914, 2480, 1005, 1055, 13954, 1997, 2852, 1012, 3897, 9221, 27895, 2015, 1037, 2843, 1997, 3241, 1998, 3980, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2065, 1996, 3737, 1997, 1996, 4405, 2003, 2151, 12407, 1997, 2054, 1005, 1055, 2664, 2000, 2272, 1010, 16428, 2213, 1998, 1996, 5541, 2136, 2024, 2275, 2000, 3613, 18667, 2290, 1005, 1055, 8027, 1997, 2034, 1011, 3446, 2547, 4730, 2007, 2178, 3040, 7699, 2580, 2547, 17743, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 12. æª¢æŸ¥ tokenization å¾Œçš„çµæœ\n",
    "\n",
    "print(val_encodings.keys())\n",
    "print(val_encodings.input_ids[0])\n",
    "print(val_encodings.attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. é€é PyTorch Dataset ä¾†å»ºç«‹èƒ½å¤ é€²è¡Œæ–¹ä¾¿è³‡æ–™å­˜å–çš„æ ¼å¼\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # Dataset class çš„ parameters æ”¾å…¥æˆ‘å€‘ tokenization å¾Œçš„è³‡æ–™ä»¥åŠè³‡æ–™çš„æ¨™ç±¤\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # è«‹æ³¨æ„ tokenization å¾Œçš„è³‡æ–™æ˜¯ä¸€å€‹ dict\n",
    "        # åœ¨æ­¤æ­¥é©Ÿå°‡è³‡æ–™ä»¥åŠæ¨™ç±¤éƒ½è½‰æ›ç‚º PyTorch çš„ tensors\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é™¤äº†è‡ªå·±è™•ç†è³‡æ–™ï¼Œä½ é‚„å¯ä»¥ä½¿ç”¨ Hugging Face Datasets\n",
    "- Hugging Face Datasets å·²ç¶“å¹«ä½ æ”¶éŒ„äº†è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸå¸¸è¦‹çš„è³‡æ–™é›†\n",
    "- ç›´æ¥å‘¼å« Datasets ä¸¦æ­é…ä¸‹é¢å¹¾å€‹ cells çš„èªæ³•ï¼Œå¯çœä¸‹ä¸å°‘æ™‚é–“\n",
    "- ä½†å‰ææ˜¯ä½ è¦é€²è¡Œçš„ä»»å‹™è³‡æ–™é›†æœ‰è¢«æ”¶éŒ„åœ¨ Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. æŸ¥çœ‹ Hugging Face Datasets çš„è³‡è¨Š\n",
    "\n",
    "datasets_list = datasets.list_datasets()\n",
    "\n",
    "print(\"ç¾åœ¨ Hugging Face Datasets æœ‰ {} å€‹è³‡æ–™é›†å¯ä»¥ä½¿ç”¨\".format(len(datasets_list)))\n",
    "print(\"===============================================\")\n",
    "# print(\"æ‰€æœ‰çš„è³‡æ–™é›†å¦‚ä¸‹: \")\n",
    "# print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/dean/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n",
      "Reusing dataset imdb (/home/dean/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    }
   ],
   "source": [
    "# 15. å¾ Hugging Face Datasets è¼‰å…¥è³‡æ–™ä¸¦åšè³‡æ–™åˆ‡åˆ†\n",
    "\n",
    "# è¼‰å…¥ IMDb çš„è¨“ç·´è³‡æ–™é›†\n",
    "train = datasets.load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "# è¨­ç«‹éš¨æ©Ÿç¨®å­ä¾†æ§åˆ¶éš¨æ©Ÿéç¨‹\n",
    "random_seed = 42\n",
    "# å¾ IMDb çš„è¨“ç·´è³‡æ–™é›†ä¸­åˆ‡åˆ†å‡ºé©—è­‰è³‡æ–™é›†\n",
    "splits = train.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=random_seed\n",
    ")\n",
    "train, valid = splits['train'], splits['test']\n",
    "\n",
    "# è¼‰å…¥ IMDb çš„æ¸¬è©¦è³‡æ–™é›†\n",
    "test = datasets.load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414f6e71ffe947fa87c1064cde6bd669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045d7e3b0adc4ed0b943cb52da920fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee09e681922041fe96429dc160ba1758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 16. å°‡ Hugging Face Datasets è½‰ç‚º PyTorch Dataset çš„å°è£\n",
    "\n",
    "def to_torch_data(hug_dataset):\n",
    "    dataset = hug_dataset.map(\n",
    "        lambda batch: tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    "    dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=[\n",
    "            'input_ids',\n",
    "            'attention_mask',\n",
    "            'label'\n",
    "        ]\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = to_torch_data(train)\n",
    "val_dataset = to_torch_data(valid)\n",
    "test_dataset = to_torch_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é€²è¡Œæ¨¡å‹çš„è¨“ç·´\n",
    "### ä½¿ç”¨ Hugging Face Trainer ([Documentation](https://huggingface.co/transformers/main_classes/trainer.html))\n",
    "- Trainer æ˜¯ Hugging Face ä¸­é«˜åº¦å°è£çš„å¥—ä»¶ä¹‹ä¸€ï¼Œè² è²¬æ¨¡å‹è¨“ç·´æ™‚æœŸçš„\"æµç¨‹\"\n",
    "- éå»æˆ‘å€‘è‡ªè¡Œå¯«è¨“ç·´æµç¨‹çš„ç¨‹å¼ç¢¼å¯ä»¥äº¤çµ¦ Trainer\n",
    "- Trainer éœ€è¦æ­é…ä½¿ç”¨ [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)\n",
    "    - TrainingArguments æ˜¯ Trainer æ‰€éœ€è¦çš„å¼•æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. å»ºç«‹è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™ (å®šç¾© function)\n",
    "# å°‡ä½œç‚º transformers.Trainer çš„ parameters ä¹‹ä¸€\n",
    "\n",
    "# Scikit-learn çš„ precision_recall_fscore_support å¥—ä»¶å¯ä»¥ä¸€æ¬¡è¨ˆç®— F1 score, precision, å’Œ recall\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 18. è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./results',          # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=3,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    per_device_train_batch_size=16,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=64,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    warmup_steps=500,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=0.01,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    logging_dir='./logs',            # å­˜æ”¾ log çš„è³‡æ–™å¤¾\n",
    "    logging_steps=10,\n",
    "    seed=random_seed\n",
    ")\n",
    "\n",
    "# åˆ©ç”¨ AutoModel å‘¼å«æ¨¡å‹\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics      # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 02:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.05041632, -0.1545653 ],\n",
       "       [-0.1051379 , -0.13048324],\n",
       "       [-0.03197539, -0.1342774 ],\n",
       "       ...,\n",
       "       [-0.08772962, -0.08445628],\n",
       "       [-0.09525743, -0.10209582],\n",
       "       [-0.093684  , -0.11937941]], dtype=float32), label_ids=array([1, 1, 1, ..., 0, 0, 0]), metrics={'test_loss': 0.694190263748169, 'test_accuracy': 0.49544, 'test_f1': 0.23542247545156986, 'test_precision': 0.48574287143571787, 'test_recall': 0.15536, 'test_runtime': 130.6797, 'test_samples_per_second': 191.308, 'init_mem_cpu_alloc_delta': -150388736, 'init_mem_gpu_alloc_delta': 268953088, 'init_mem_cpu_peaked_delta': 150388736, 'init_mem_gpu_peaked_delta': 0, 'test_mem_cpu_alloc_delta': -24576, 'test_mem_gpu_alloc_delta': 0, 'test_mem_cpu_peaked_delta': 24576, 'test_mem_gpu_peaked_delta': 2316313600})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 19. æ¸¬è©¦æ¨¡å‹\n",
    "\n",
    "trainer.predict(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
