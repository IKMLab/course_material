{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT tutorial using Hugging Face\n",
    "## æ•™å­¸ç›®æ¨™\n",
    "åˆ©ç”¨ Hugging Face å¥—ä»¶å¿«é€Ÿä½¿ç”¨ BERT æ¨¡å‹ä¾†é€²è¡Œä¸‹æ¸¸ä»»å‹™è¨“ç·´\n",
    "- å–®ä¸€å¥å‹åˆ†é¡ä»»å‹™ (single-sentence text classification)\n",
    "- å•ç­”ä»»å‹™ (question answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face ä»‹ç´¹\n",
    "- ğŸ¤— Hugging Face æ˜¯å°ˆé–€æä¾›è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸçš„å‡½å¼åº«\n",
    "- å…¶å‡½å¼åº«æ”¯æ´ PyTorch å’Œ TensorFlow\n",
    "- ğŸ¤— Hugging Face çš„ä¸»è¦å¥—ä»¶ç‚º:\n",
    "    1. Transformers ([é€£çµ](https://huggingface.co/transformers/index.html))\n",
    "    - æä¾›äº†ç¾ä»Šæœ€å¼·å¤§çš„è‡ªç„¶èªè¨€è™•ç†æ¨¡å‹ï¼Œä½¿ç”¨ä¸Šéå¸¸å½ˆæ€§ä¸”æ–¹ä¾¿\n",
    "    2. Tokenizers ([é€£çµ](https://huggingface.co/docs/tokenizers/python/latest/))\n",
    "    - è®“ä½ å¯ä»¥å¿«é€Ÿåšå¥½ BERT ç³»åˆ—æ¨¡å‹ tokenization\n",
    "    3. Datasets ([é€£çµ](https://huggingface.co/docs/datasets/))\n",
    "    - æä¾›å¤šç¨®è‡ªç„¶èªè¨€è™•ç†ä»»å‹™çš„è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch çš„ç‰ˆæœ¬ç‚º: 1.7.1\n",
      "Hugging Face Transformers çš„ç‰ˆæœ¬ç‚º: 4.5.1\n",
      "Hugging Face Datasets çš„ç‰ˆæœ¬ç‚º: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch çš„ç‰ˆæœ¬ç‚º: {}\".format(torch.__version__))\n",
    "\n",
    "import transformers\n",
    "print(\"Hugging Face Transformers çš„ç‰ˆæœ¬ç‚º: {}\".format(transformers.__version__))\n",
    "\n",
    "import datasets\n",
    "print(\"Hugging Face Datasets çš„ç‰ˆæœ¬ç‚º: {}\".format(datasets.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å–®ä¸€å¥å‹åˆ†é¡ä»»å‹™ (single-sentence text classification)\n",
    "## æº–å‚™è³‡æ–™é›† (éœ€å…ˆä¸‹è¼‰)\n",
    "æˆ‘å€‘ä½¿ç”¨ IMDb reviews è³‡æ–™é›†ä½œç‚ºç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/dean/.local/share/virtualenvs/nlp-tutorial-pytorch-S7P7VhMA/lib/python3.8/site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install wget\n",
    "import wget\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filename = wget.download(url, out='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tarfile\n",
    "import tarfile\n",
    "\n",
    "# æŒ‡å®šæª”æ¡ˆä½ç½®ï¼Œä¸¦è§£å£“ç¸® .gz çµå°¾çš„å£“ç¸®æª”\n",
    "tar = tarfile.open('aclImdb_v1.tar.gz', 'r:gz')\n",
    "tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¥ä¸‹ä¾†æˆ‘å€‘è¦é€²è¡Œè³‡æ–™å‰è™•ç†\n",
    "ä½†é¦–å…ˆè¦è§€å¯Ÿè§£å£“ç¸®å¾Œçš„è³‡æ–™å¤¾çµæ§‹:\n",
    "```\n",
    "aclImdb---\n",
    "        |--train\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--test\n",
    "        |    |--neg\n",
    "        |    |--pos\n",
    "        |    |--...\n",
    "        |--imdb.vocab\n",
    "        |--imdbEr.text\n",
    "        |--README\n",
    "```\n",
    "å…¶ä¸­ train å’Œ test è³‡æ–™å¤¾ä¸­åˆ†åˆ¥åˆæœ‰ neg å’Œ pos å…©ç¨®è³‡æ–™å¤¾\n",
    "\n",
    "æˆ‘å€‘è¦é‡å°é€™å…©å€‹ç›®æ¨™è³‡æ–™å¤¾é€²è¡Œè™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ pathlib æ¨¡çµ„ (Python3.4+)\n",
    "from pathlib import Path\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        # åˆ©ç”¨iterdir() ä¾†åˆ—å‡ºè³‡æ–™å¤¾åº•ä¸‹çš„æ‰€æœ‰æª”æ¡ˆï¼Œæ­¤åŠŸèƒ½ç­‰åŒæ–¼ os.path.listdir()\n",
    "#         for text_file in (split_dir/label_dir).iterdir():\n",
    "        # è‹¥çŸ¥é“å‰¯æª”åç‚º.txt\n",
    "        for text_file in (split_dir/label_dir).glob(\"*.txt\"):\n",
    "            tmp_text = text_file.read_text()\n",
    "#             print(tmp_text)\n",
    "#             exit()\n",
    "            texts.append(tmp_text)\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)\n",
    "    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ‡åˆ†è¨“ç·´è³‡æ–™ï¼Œä¾†åˆ†å‡º validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# è¨­ç«‹éš¨æ©Ÿç¨®å­ä¾†æ§åˆ¶éš¨æ©Ÿéç¨‹\n",
    "random_seed = 42\n",
    "\n",
    "# è¨­å®šè¦åˆ†å‡ºå¤šå°‘æ¯”ä¾‹çš„ validation data\n",
    "valid_ratio = 0.2\n",
    "\n",
    "# ä½¿ç”¨ train_test_split ä¾†åˆ‡åˆ†è³‡æ–™\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=valid_ratio, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizization\n",
    "- æ–·å­—çš„éƒ¨ä»½ä»¥ DistilBERT (Sanh et al., 2019) çš„ tokenizer ç‚ºä¾‹\n",
    "- Hugging Face çš„ tokenizer å¯ä»¥ç›´æ¥å¹«ä½ è‡ªå‹•å°‡è³‡æ–™è½‰æ›æˆ BERT çš„è¼¸å…¥å‹å¼ (ä¹Ÿå°±æ˜¯åŠ å…¥[CLS]å’Œ[SEP] tokens)\n",
    "- æ¥è‘—ç”±æ–¼æ·±åº¦å­¸ç¿’ç¶²è·¯éœ€è¦ä½¿ç”¨ mini-batch é€²è¡Œå­¸ç¿’ï¼Œæˆ‘å€‘éœ€è¦å…ˆä»¥ PyTorch dataset ä¾†è‡ªè¡Œå»ºç«‹å°è£è³‡æ–™çš„ç‰©ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨ Hugging Face å¥—ä»¶ä¸­å¯ä½¿ç”¨ .from_pretrained() çš„æ–¹æ³•ä¾†å°å…¥é è¨“ç·´æ¨¡å‹\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†åˆ¥å°‡3ç¨®è³‡æ–™ (train/valid/test) åš tokenization\n",
    "# truncation ä»£è¡¨\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = datasets.list_datasets()\n",
    "\n",
    "print(\"ç¾åœ¨ Hugging Face Datasets æœ‰ {} å€‹è³‡æ–™é›†å¯ä»¥ä½¿ç”¨\".format(len(datasets_list)))\n",
    "print(\"===============================================\")\n",
    "print(\"æ‰€æœ‰çš„è³‡æ–™é›†å¦‚ä¸‹: \")\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/dean/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n",
      "Reusing dataset imdb (/home/dean/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    }
   ],
   "source": [
    "# è¨­ç«‹éš¨æ©Ÿç¨®å­ä¾†æ§åˆ¶éš¨æ©Ÿéç¨‹\n",
    "random_seed = 42\n",
    "\n",
    "train = datasets.load_dataset(\"imdb\", split=\"train\")\n",
    "splits = train.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=random_seed\n",
    ")\n",
    "train, valid = splits['train'], splits['test']\n",
    "\n",
    "test = datasets.load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch_data(hug_dataset):\n",
    "    dataset = hug_dataset.map(\n",
    "        lambda batch: tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "    dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=[\n",
    "            'input_ids',\n",
    "            'attention_mask',\n",
    "            'label'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7039928e4eb544e2b46030148b176a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36d4d0ba0b94307b674eb13a7be964d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb20b91db80a42428df5fa1934331159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = to_torch_data(train)\n",
    "val_dataset = to_torch_data(valid)\n",
    "test_dataset = to_torch_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    ")\n",
    "\n",
    "# trainer.is_model_parallel=True\n",
    "trainer.args._n_gpu=1\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 02:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.12364519, -0.05538433],\n",
       "       [-0.09750886, -0.08097227],\n",
       "       [-0.14379609, -0.05519198],\n",
       "       ...,\n",
       "       [-0.06995446, -0.06064164],\n",
       "       [-0.11827329, -0.04072288],\n",
       "       [-0.13526428, -0.09345569]], dtype=float32), label_ids=array([1, 1, 1, ..., 0, 0, 0]), metrics={'test_loss': 0.6959018111228943, 'test_runtime': 127.6522, 'test_samples_per_second': 195.845, 'init_mem_cpu_alloc_delta': 2148904960, 'init_mem_gpu_alloc_delta': 268953088, 'init_mem_cpu_peaked_delta': 199368704, 'init_mem_gpu_peaked_delta': 0, 'test_mem_cpu_alloc_delta': 15785984, 'test_mem_gpu_alloc_delta': 0, 'test_mem_cpu_peaked_delta': 1085440, 'test_mem_gpu_peaked_delta': 2316313600})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å•ç­”ä»»å‹™ (question answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æº–å‚™è³‡æ–™é›† (éœ€å…ˆä¸‹è¼‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ Squad çš„è³‡æ–™å¤¾\n",
    "if not os.path.exists(\"./squad\"):\n",
    "    os.mkdir(\"./squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !pip install wget\n",
    "import wget\n",
    "\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'\n",
    "filename = wget.download(url, out='./squad/')\n",
    "\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "filename = wget.download(url, out='./squad/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two â€“ fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = transformers.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
